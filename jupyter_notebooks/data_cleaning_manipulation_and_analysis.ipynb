{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# **Data Cleaning, Manipulation and Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## **Objectives**\n",
    "\n",
    "The intention of this notebook was tri-fold: data cleaning, data transformation and data loading. Light analysis has also been carried out to better understand, extract and load data.\n",
    "\n",
    "### **Inputs**\n",
    "\n",
    "* Dataset retrieved from Kaggle (CSV file containing data regarding patients with, or potentially at risk of, Alzheimer's disease saved to inputs folder)\n",
    "\n",
    "### **Outputs**\n",
    "\n",
    "* Data cleaning pipeline (within this notebook)\n",
    "* Machine learning pipeline (within this notebook)\n",
    "* Cleaned data (csv file extracted to outputs folder)\n",
    "* Data for machine learning (txt file extracted to outputs folder)\n",
    "\n",
    "### **Additional Comments**\n",
    "\n",
    "* Data was extracted from Kaggle with the source citation included in the README file.\n",
    "* Data was saved in its raw orginal form and then cleaned (a machine learning dataset with scaling and encoding was also created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **REMINDER**: \n",
    "All notebook cells should be run top-down (you can't create a dynamic where at a given point you need to go back to a previous cell to execute a task and then return to the cell you were working on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **IMPORTANT**: \n",
    "Before running the cells below, you **MUST** restart the kernel!\n",
    "\n",
    "**This is because:**\n",
    "- Windows locks files that are currently in use.\n",
    "- NumPy is loaded in the current kernel session.\n",
    "- Restarting clears memory and releases file locks.\n",
    "\n",
    "**How to restart the kernel:**\n",
    "1. Click on the restart button above with the circular arrow before it\n",
    "2. Confirm the restart\n",
    "3. **Then** run the cells below in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "### **Change Working Directory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When storing the notebooks in a subfolder to run in the editor, for projects such as these, it's best practice to change the working directory. \n",
    "* We need to change the working directory from its current folder to its parent folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\F_bee\\\\Documents\\\\vs-code\\\\vs-code-projects\\\\healthcare-and-public-health\\\\jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the current directory with os.getcwd()\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "# Make parent of current directory the new current directory\n",
    "# Use os.path.dirname() to get parent directory\n",
    "# Use os.chdir() to define new current directory\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\F_bee\\\\Documents\\\\vs-code\\\\vs-code-projects\\\\healthcare-and-public-health'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm new current directory\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Install Packages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (2.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Upgrade numpy (run after kernel restart)\n",
    "%pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: seaborn in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (6.2.0)\n",
      "Requirement already satisfied: feature-engine in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (1.8.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from plotly) (2.0.1)\n",
      "Requirement already satisfied: statsmodels>=0.11.1 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from feature-engine) (0.14.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\f_bee\\documents\\vs-code\\vs-code-projects\\healthcare-and-public-health\\.venv\\lib\\site-packages (from statsmodels>=0.11.1->feature-engine) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install other packages (run after numpy upgrade)\n",
    "%pip install pandas matplotlib seaborn scikit-learn plotly feature-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all imports (run after all packages installed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mb\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as pl\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import feature_engine as fe\n",
    "\n",
    "print(\"All packages imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {mb.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")\n",
    "print(f\"Scikit-learn version: {sk.__version__}\")\n",
    "print(f\"Plotly version: {pl.__version__}\")\n",
    "print(f\"Feature-engine version: {fe.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Extraction**\n",
    "This section contains code for the loading of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the dataset from the inputs folder and load it to notebook as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"inputs/alzheimers_disease_data.csv\")\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random sample of the data. Consider the first 5 rows (head) throughout for better notebook observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.25, random_state=10)\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY3l0-AxO93d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFQo3ycuO-v6"
   },
   "source": [
    "## **Section 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Transformation**\n",
    "This section contains functions for transformer creation, pipeline code and light analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the current columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data loaded successfully!\")\n",
    "print(\"Available columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the minimum values for numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data loaded successfully!\")\n",
    "numerical_columns = [\"Age\", \"Gender\", \"Ethnicity\", \"EducationLevel\", \"BMI\", \"AlcoholConsumption\", \"PhysicalActivity\", \"DietQuality\", \"SleepQuality\", \"SystolicBP\", \"DiastolicBP\", \"CholesterolTotal\", \"CholesterolLDL\", \"CholesterolHDL\", \"CholesterolTriglycerides\", \"MMSE\", \"FunctionalAssessment\"]\n",
    "print(\"Minimum values for numerical columns:\")\n",
    "df[numerical_columns].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the maximum values for numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data loaded successfully!\")\n",
    "numerical_columns = [\"Age\", \"Gender\", \"Ethnicity\", \"EducationLevel\", \"BMI\", \"AlcoholConsumption\", \"PhysicalActivity\", \"DietQuality\", \"SleepQuality\", \"SystolicBP\", \"DiastolicBP\", \"CholesterolTotal\", \"CholesterolLDL\", \"CholesterolHDL\", \"CholesterolTriglycerides\", \"MMSE\", \"FunctionalAssessment\"]\n",
    "print(\"Maximum values for numerical columns:\")\n",
    "df[numerical_columns].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicates and retrieve their sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data loaded successfully!\")\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for null values and retrieve their sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data loaded successfully!\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create code to populate categorical columns with integer values with their string counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data loaded successfully!\")\n",
    "# Replace values in categorical columns for better readability\n",
    "# Gender mapping\n",
    "if \"Gender\" in df.columns:\n",
    "    df[\"Gender\"] = df[\"Gender\"].replace({0: \"Male\", 1: \"Female\"})\n",
    "\n",
    "# Ethnicity mapping\n",
    "if \"Ethnicity\" in df.columns:\n",
    "    df[\"Ethnicity\"] = df[\"Ethnicity\"].replace({\n",
    "        0: \"Caucasian\", 1: \"African American\", 2: \"Asian\", 3: \"Other\"\n",
    "    })\n",
    "\n",
    "# Binary columns (0/1 to No/Yes)\n",
    "binary_cols = [\"Smoking\", \"CardiovascularDisease\", \"Depression\", \n",
    "               \"MemoryComplaints\", \"BehavioralProblems\", \"PersonalityChanges\", \n",
    "               \"DifficultyCompletingTasks\"]\n",
    "\n",
    "for col in binary_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace({0: \"No\", 1: \"Yes\"})\n",
    "\n",
    "# Diagnosis mapping\n",
    "if \"Diagnosis\" in df.columns:\n",
    "    df[\"Diagnosis\"] = df[\"Diagnosis\"].replace({0: \"No Dementia\", 1: \"Dementia\"})\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to load into transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop specific columns\n",
    "def drop_columns(df):\n",
    "    return df.drop(columns=[\"EducationLevel\", \"SleepQuality\", \"FamilyHistoryAlzheimers\", \"Diabetes\", \"HeadInjury\", \"Hypertension\", \"SystolicBP\", \"DiastolicBP\", \"CholesterolLDL\", \"CholesterolHDL\", \"CholesterolTriglycerides\", \"Confusion\", \"Disorientation\", \"Forgetfulness\", \"DoctorInCharge\"], errors=\"ignore\")\n",
    "\n",
    "# Change column locations\n",
    "def change_column_location(df):\n",
    "    new_column_order = [\"PatientID\", \"Age\", \"Gender\", \"Ethnicity\", \"BMI\", \"DietQuality\", \"PhysicalActivity\", \"Smoking\", \"AlcoholConsumption\", \"CardiovascularDisease\", \"CholesterolTotal\", \"FunctionalAssessment\", \"ADL\", \"MMSE\", \"MemoryComplaints\", \"BehavioralProblems\", \"PersonalityChanges\", \"DifficultyCompletingTasks\", \"Depression\", \"Diagnosis\"]\n",
    "    # Only include columns that actually exist in the dataframe\n",
    "    existing_columns = [col for col in new_column_order if col in df.columns]\n",
    "    return df[existing_columns]\n",
    "\n",
    "# Convert data types\n",
    "def convert_data_types(df):\n",
    "    if \"PatientID\" in df.columns:\n",
    "        df[\"PatientID\"] = df[\"PatientID\"].astype(int)\n",
    "    if \"Age\" in df.columns:\n",
    "        df[\"Age\"] = df[\"Age\"].astype(int)\n",
    "    if \"Gender\" in df.columns:\n",
    "        df[\"Gender\"] = df[\"Gender\"].astype(str)\n",
    "    if \"Ethnicity\" in df.columns:\n",
    "        df[\"Ethnicity\"] = df[\"Ethnicity\"].astype(str)\n",
    "    if \"BMI\" in df.columns:\n",
    "        df[\"BMI\"] = df[\"BMI\"].astype(float)\n",
    "    if \"Smoking\" in df.columns:\n",
    "        df[\"Smoking\"] = df[\"Smoking\"].astype(str)\n",
    "    if \"AlcoholConsumption\" in df.columns:\n",
    "        df[\"AlcoholConsumption\"] = df[\"AlcoholConsumption\"].astype(float)\n",
    "    if \"PhysicalActivity\" in df.columns:\n",
    "        df[\"PhysicalActivity\"] = df[\"PhysicalActivity\"].astype(int)\n",
    "    if \"DietQuality\" in df.columns:\n",
    "        df[\"DietQuality\"] = df[\"DietQuality\"].astype(str)\n",
    "    if \"CardiovascularDisease\" in df.columns:\n",
    "        df[\"CardiovascularDisease\"] = df[\"CardiovascularDisease\"].astype(str)\n",
    "    if \"Depression\" in df.columns:\n",
    "        df[\"Depression\"] = df[\"Depression\"].astype(str)\n",
    "    if \"CholesterolTotal\" in df.columns:\n",
    "        df[\"CholesterolTotal\"] = df[\"CholesterolTotal\"].astype(float)\n",
    "    if \"MMSE\" in df.columns:\n",
    "        df[\"MMSE\"] = df[\"MMSE\"].astype(float)\n",
    "    if \"FunctionalAssessment\" in df.columns:\n",
    "        df[\"FunctionalAssessment\"] = df[\"FunctionalAssessment\"].astype(int)\n",
    "    if \"MemoryComplaints\" in df.columns:\n",
    "        df[\"MemoryComplaints\"] = df[\"MemoryComplaints\"].astype(str)\n",
    "    if \"BehavioralProblems\" in df.columns:\n",
    "        df[\"BehavioralProblems\"] = df[\"BehavioralProblems\"].astype(str)\n",
    "    if \"ADL\" in df.columns:\n",
    "        df[\"ADL\"] = df[\"ADL\"].astype(float)\n",
    "    if \"PersonalityChanges\" in df.columns:\n",
    "        df[\"PersonalityChanges\"] = df[\"PersonalityChanges\"].astype(str)\n",
    "    if \"DifficultyCompletingTasks\" in df.columns:\n",
    "        df[\"DifficultyCompletingTasks\"] = df[\"DifficultyCompletingTasks\"].astype(str)\n",
    "    if \"Diagnosis\" in df.columns:\n",
    "        df[\"Diagnosis\"] = df[\"Diagnosis\"].astype(str)\n",
    "    return df\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "def remove_outliers(df):\n",
    "    columns = [\"BMI\", \"CholesterolTotal\"]\n",
    "    df_cleaned = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df_cleaned.columns: \n",
    "            Q1 = df_cleaned[col].quantile(0.25)\n",
    "            Q3 = df_cleaned[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            mask = (df_cleaned[col] >= Q1 - 1.5 * IQR) & (df_cleaned[col] <= Q3 + 1.5 * IQR)\n",
    "            df_cleaned = df_cleaned[mask]  \n",
    "    return df_cleaned\n",
    "\n",
    "# Scale numerical values and encode categorical values\n",
    "scaling_transformer = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), [\"Patient_Age\", \"BMI\", \"Alcohol_Consumption\", \"Physical_Activity\", \"Cholesterol_Total\", \"MMSE\", \"Functional_Assessment\", \"Activities_Of_Daily_Living\"]), \n",
    "    (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), [\"Gender\", \"Ethnicity\", \"Smoking\", \"Cardiovascular_Disease\", \"Depression\", \"Memory_Complaints\", \"Behavioral_Problems\", \"Personality_Changes\", \"Difficulty_Completing_Tasks\"])  \n",
    "])\n",
    "\n",
    "# Rename columns\n",
    "def rename_columns(df):\n",
    "    return df.rename(columns={\n",
    "        \"PatientID\": \"Patient_ID\",\n",
    "        \"Age\": \"Patient_Age\",\n",
    "        \"AlcoholConsumption\": \"Alcohol_Consumption\",\n",
    "        \"PhysicalActivity\": \"Physical_Activity\",\n",
    "        \"DietQuality\": \"Diet_Quality\",\n",
    "        \"CardiovascularDisease\": \"Cardiovascular_Disease\",\n",
    "        \"CholesterolTotal\": \"Cholesterol_Total\",\n",
    "        \"FunctionalAssessment\": \"Functional_Assessment\",\n",
    "        \"MemoryComplaints\": \"Memory_Complaints\",\n",
    "        \"BehavioralProblems\": \"Behavioral_Problems\",\n",
    "        \"ADL\": \"Activities_Of_Daily_Living\",\n",
    "        \"PersonalityChanges\": \"Personality_Changes\",\n",
    "        \"DifficultyCompletingTasks\": \"Difficulty_Completing_Tasks\",  \n",
    "    })\n",
    "\n",
    "# Drop missing values\n",
    "def drop_missing_values(df):\n",
    "    return df.dropna()\n",
    "\n",
    "# Remove duplicates\n",
    "def remove_duplicates(df):\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "# Round numerical values to 2 decimal places\n",
    "def round_values(df):\n",
    "    return df.round(2)\n",
    "\n",
    "# Capitalize column names with proper acronym handling\n",
    "def capitalize_columns(df):\n",
    "    def smart_title(text):\n",
    "        # Common acronyms that should stay uppercase\n",
    "        acronyms = {\n",
    "            \"bmi\": \"BMI\",\n",
    "            \"mmse\": \"MMSE\", \n",
    "            \"adl\": \"ADL\",\n",
    "            \"id\": \"ID\"\n",
    "        }\n",
    "        \n",
    "        # Split by underscore and process each part\n",
    "        parts = text.split(\"_\")\n",
    "        result_parts = []\n",
    "        \n",
    "        for part in parts:\n",
    "            lower_part = part.lower()\n",
    "            if lower_part in acronyms:\n",
    "                result_parts.append(acronyms[lower_part])\n",
    "            else:\n",
    "                result_parts.append(part.title())\n",
    "        \n",
    "        return \"_\".join(result_parts)\n",
    "    \n",
    "    df.columns = [smart_title(col) for col in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformers\n",
    "change_column_location_transformer = FunctionTransformer(change_column_location)\n",
    "drop_columns_transformer = FunctionTransformer(drop_columns)\n",
    "convert_data_types_transformer = FunctionTransformer(convert_data_types)\n",
    "remove_outliers_transformer = FunctionTransformer(remove_outliers)\n",
    "rename_columns_transformer = FunctionTransformer(rename_columns)\n",
    "capitalize_columns_transformer = FunctionTransformer(capitalize_columns)\n",
    "drop_missing_values_transformer = FunctionTransformer(drop_missing_values)\n",
    "remove_duplicates_transformer = FunctionTransformer(remove_duplicates)\n",
    "round_values_transformer = FunctionTransformer(round_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data cleaning pipeline\n",
    "data_cleaning_pipeline = Pipeline([\n",
    "    (\"drop_columns\", drop_columns_transformer),\n",
    "    (\"change_column_order\", change_column_location_transformer),\n",
    "    (\"convert_data_types\", convert_data_types_transformer),\n",
    "    (\"rename_columns\", rename_columns_transformer),\n",
    "    (\"capitalize_columns\", capitalize_columns_transformer),\n",
    "    (\"remove_outliers\", remove_outliers_transformer),\n",
    "    (\"drop_missing_values\", drop_missing_values_transformer),\n",
    "    (\"remove_duplicates\", remove_duplicates_transformer),\n",
    "    (\"round_values\", round_values_transformer)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create advanced machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced pipeline with scaling and encoding for machine learning\n",
    "# This pipeline should clean and preprocess data, rename columns, scale numerical features, encode categorical features and handle unknown categories\n",
    "data_cleaning_with_ml_pipeline = Pipeline([\n",
    "    (\"drop_columns\", drop_columns_transformer),\n",
    "    (\"change_column_order\", change_column_location_transformer),\n",
    "    (\"convert_data_types\", convert_data_types_transformer),\n",
    "    (\"rename_columns\", rename_columns_transformer),\n",
    "    (\"capitalize_columns\", capitalize_columns_transformer),\n",
    "    (\"remove_outliers\", remove_outliers_transformer),\n",
    "    (\"drop_missing_values\", drop_missing_values_transformer),\n",
    "    (\"remove_duplicates\", remove_duplicates_transformer),\n",
    "    (\"round_values\", round_values_transformer),  \n",
    "    (\"scale_and_encode\", scaling_transformer)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Loading** \n",
    "In this section, we fit both pipelines to two separate instances of the same dataframe, allowing for the transformation process to take place, and the creation of new datasets due for loading as new, cleaned CSV and txt documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the pipeline to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to original dataframe\n",
    "processed_df = data_cleaning_pipeline.fit_transform(df)\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Processed data shape: {processed_df.shape}\")\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the current column list after fitting pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data loaded successfully!\")\n",
    "print(\"Processed DataFrame columns:\", processed_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit the machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the ML pipeline to original dataframe\n",
    "scaled_encoded_df = data_cleaning_with_ml_pipeline.fit_transform(df)\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Scaled data shape: {scaled_encoded_df.shape}\")\n",
    "print(scaled_encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load both previously created dataframes to separate CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed datasets\n",
    "processed_df.to_csv(\"outputs/processed_alzheimers_disease_data_unscaled_and_unencoded.csv\", index=False)\n",
    "np.savetxt(\"outputs/processed_alzheimers_disease_data_scaled_and_encoded.csv\", \n",
    "           scaled_encoded_df, delimiter=\",\", fmt=\"%.6f\")\n",
    "print(\"Files saved to outputs folder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltNetd085qHf"
   },
   "source": [
    "## **Conclusion**\n",
    "The process approached with some difficulty, but in the end, we managed to generate the instances of the datasets we were after. These will then be used within our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Notes** \n",
    "**Method**\n",
    "- Created the mapping and binary columns as the original data was populated with numerical values for all columns.\n",
    "- Dropped several columns as aim was to focus on partiuclar parameters (health and lifestyle) and to afford a more simplistic, less technical, viewer/user friendly application.\n",
    "- Also extracted a fractioned/sampled DataFrame from the original at a random state for analysis purposes.\n",
    "\n",
    "**Issues**\n",
    "- Following package conflicts with packages like NumPy and Pandas the installation block was added as a precautionary measure.\n",
    "- Needed to install Jupyter dependencies within the notebook, as kernel kept dying; Python kernel was restarted, then the necessary packages were downloaded.\n",
    "- Pandas faced issues such as import errors and HTML errors; this was resolved via a Pandas update as well as using the print function. \n",
    "\n",
    "**Further Considerations**\n",
    "- Consider not to rule out further factors, such head injury, other potential comordid diseases such as diabetes, or family history (these were included in the orginal dataset).\n",
    "- Capitalize, drop missing values and remove duplicates were added for quality assurance purposes."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
